# Contiv-VPP deployment YAML file. This deploys Contiv VPP networking on a Kuberntes cluster.
# The deployment consists of the following components:
#   - contiv-etcd - deployed on k8s master
#   - contiv-vswitch - deployed on each k8s node
#   - contiv-ksr - deployed on k8s master

###########################################################
#  Configuration
###########################################################

# This config map contains contiv-agent configuration. The most important part is the IPAMConfig,
# which may be updated in case the default IPAM settings do not match your needs.
# NodeConfig may be used in case your nodes have more than 1 VPP interface. In that case, one
# of them needs to be marked as the main inter-node interface, and the rest of them can be
# configured with any IP addresses (the IPs cannot conflict with the main IPAM config).
apiVersion: v1
kind: ConfigMap
metadata:
  name: contiv-agent-cfg
  namespace: kube-system
data:
  contiv.yaml: |-
    TCPstackDisabled: {{ .Values.contiv.tcpStackDisabled }}
    UseTAPInterfaces: {{ .Values.contiv.useTAPInterfaces }}
    TAPInterfaceVersion: {{ .Values.contiv.tapInterfaceVersion }}
    {{- if eq (.Values.contiv.tapInterfaceVersion | int) 2 }}
    TAPv2RxRingSize: {{ .Values.contiv.tapv2RxRingSize }}
    TAPv2TxRingSize: {{ .Values.contiv.tapv2TxRingSize }}
    {{- end }}
    {{- if .Values.contiv.stealInterface }}
    StealInterface: {{ .Values.contiv.stealInterface }}
    {{- end }}
    {{- if .Values.contiv.stealFirstNIC }}
    StealFirstNIC: True
    {{- end }}
    NatExternalTraffic: {{ .Values.contiv.natExternalTraffic }}
    MTUSize: {{ .Values.contiv.mtuSize }}
    {{- if .Values.contiv.cleanupIdleNATSessions }}
    CleanupIdleNATSessions: True
    TCPNATSessionTimeout: {{ .Values.contiv.tcpNATSessionTimeout }}
    OtherNATSessionTimeout: {{ .Values.contiv.otherNATSessionTimeout }}
    {{- end }}
    ScanIPNeighbors: {{ .Values.contiv.scanIPNeighbors }}
    IPNeighborScanInterval: {{ .Values.contiv.ipNeighborScanInterval }}
    IPNeighborStaleThreshold: {{ .Values.contiv.ipNeighborStaleThreshold }}
    {{- if .Values.contiv.serviceLocalEndpointWeight }}
    ServiceLocalEndpointWeight: {{ .Values.contiv.serviceLocalEndpointWeight }}
    {{- end }}
    DisableNATVirtualReassembly: {{ .Values.contiv.disableNATVirtualReassembly }}
    EnablePacketTrace: {{ .Values.contiv.enablePacketTrace }}
    CRDNodeConfigurationDisabled: {{ .Values.contiv.crdNodeConfigurationDisabled }}
    IPAMConfig:
      NodeInterconnectDHCP: {{ .Values.contiv.ipamConfig.nodeInterconnectDHCP }}
      {{- if .Values.contiv.ipamConfig.nodeInterconnectCIDR }}
      NodeInterconnectCIDR: {{ .Values.contiv.ipamConfig.nodeInterconnectCIDR }}
      {{- end }}
      {{- if .Values.contiv.ipamConfig.serviceCIDR }}
      ServiceCIDR: {{ .Values.contiv.ipamConfig.serviceCIDR }}
      {{- end }}
      {{- if .Values.contiv.ipamConfig.contivCIDR }}
      ContivCIDR: {{ .Values.contiv.ipamConfig.contivCIDR }}
      {{ else }}
      PodSubnetCIDR: {{ .Values.contiv.ipamConfig.podSubnetCIDR }}
      PodNetworkPrefixLen: {{ .Values.contiv.ipamConfig.podNetworkPrefixLen }}
      PodIfIPCIDR: {{ .Values.contiv.ipamConfig.podIfIPCIDR }}
      VPPHostSubnetCIDR: {{ .Values.contiv.ipamConfig.vppHostSubnetCIDR }}
      VPPHostNetworkPrefixLen: {{ .Values.contiv.ipamConfig.vppHostNetworkPrefixLen }}
      VxlanCIDR: {{ .Values.contiv.ipamConfig.vxlanCIDR }}
      {{- end }}
    {{- if .Values.contiv.nodeConfig }}
    NodeConfig:
    {{- range .Values.contiv.nodeConfig }}
    - NodeName: {{ .name }}
      MainVppInterface:
        InterfaceName: {{ .mainInterface.interfaceName }}
        {{- if .mainInterface.useDHCP }}
        UseDHCP: {{ .mainInterface.useDHCP }}
        {{- end -}}
        {{- if .mainInterface.ip }}
        IP: {{ .mainInterface.ip }}
        {{- end -}}
      {{- if .natExternalTraffic }}
      NatExternalTraffic: {{ .natExternalTraffic }}
      {{- end }}
      {{- if .gateway }}
      Gateway: {{ .gateway }}
      {{- end -}}
      {{- if .stealInterface }}
      StealInterface: {{ .stealInterface }}
      {{- end -}}
      {{- if .otherInterfaces }}
      OtherVPPInterfaces:
      {{- range $iface := .otherInterfaces }}
        - InterfaceName: {{ $iface.interfaceName }}
          IP: {{ $iface.ip }}
      {{- end }}
    {{- end }}
    {{- end }}
    {{- end }}
  logs.conf: |
    default-level: {{ .Values.logs.defaultLevel }}
    loggers:
      - name: statscollector
        level: info
      - name: vpp.if-state
        level: info
  grpc.conf: |
    network: unix
    endpoint: /var/run/contiv/cni.sock
    force-socket-removal: true
    permission: 700
  http.conf: |
    endpoint: 0.0.0.0:9999
    {{- if .Values.http.enableServerCert }}
    server-cert-file: /var/http/{{ .Values.http.serverCert }}
    server-key-file: /var/http/{{ .Values.http.serverKey }}
    {{- end }}
    {{- if .Values.http.enableBasicAuth }}
    client-basic-auth:
      - {{ .Values.http.basicAuth | quote }}
    {{- end }}
  bolt.conf: |
    db-path: /var/bolt/bolt.db
    file-mode: 432
    lock-timeout: 0
  telemetry.conf: |
    polling-interval: {{ .Values.telemetry.pollingInterval | int64 }}
    disabled: {{ .Values.telemetry.disabled }}

---

apiVersion: v1
kind: ConfigMap
metadata:
  name: govpp-cfg
  namespace: kube-system
data:
  govpp.conf: |
    health-check-probe-interval: {{ .Values.govpp.healthCheckProbeInterval | int64 }}
    health-check-reply-timeout: {{ .Values.govpp.healthCheckReplyTimeout | int64 }}
    health-check-threshold: {{ .Values.govpp.healthCheckThreshold | int64 }}
    reply-timeout: {{ .Values.govpp.replyTimeout | int64 }}

---

###########################################################
#
# !!! DO NOT EDIT THINGS BELOW THIS LINE !!!
#
###########################################################


###########################################################
#  Components and other resources
###########################################################

# This installs the contiv-etcd (ETCD server to be used by Contiv) on the master node in a Kubernetes cluster.
# In odrer to dump the content of ETCD, you can use the kubectl exec command similar to this:
#   kubectl exec contiv-etcd-cxqhr -n kube-system etcdctl -- get --endpoints=[127.0.0.1:12379] --prefix="true" ""
apiVersion: apps/v1beta2
kind: StatefulSet
metadata:
  name: contiv-etcd
  namespace: kube-system
  labels:
    k8s-app: contiv-etcd
spec:
  serviceName: contiv-etcd
  selector:
    matchLabels:
      k8s-app: contiv-etcd
  updateStrategy:
    type: {{ .Values.etcd.updateStrategy }}
  template:
    metadata:
      labels:
        k8s-app: contiv-etcd
      annotations:
        # Marks this pod as a critical add-on.
        scheduler.alpha.kubernetes.io/critical-pod: ''
    spec:
      tolerations:
      # We need this to schedule on the master no matter what else is going on, so tolerate everything.
      - key: ''
        operator: Exists
        effect: ''
      # This likely isn't needed due to the above wildcard, but keep it in for now.
      - key: CriticalAddonsOnly
        operator: Exists
      # Only run this pod on the master.
      nodeSelector:
        node-role.kubernetes.io/master: ""
      hostNetwork: true

      containers:
        - name: contiv-etcd
          {{- if .Values.Arm64Platform }}
          image: {{ .Values.etcd_arm64.image.repository }}:{{ .Values.etcd_arm64.image.tag }}
          {{- else }}
          image: {{ .Values.etcd.image.repository }}:{{ .Values.etcd.image.tag }}
          {{- end }}
          imagePullPolicy: {{ .Values.etcd.image.pullPolicy }}
          env:
            - name: CONTIV_ETCD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: ETCDCTL_API
              value: "3"
            {{- if .Values.Arm64Platform }}
            - name: ETCD_UNSUPPORTED_ARCH
              value: "arm64"
            {{- end }}
            {{- if .Values.etcd.secureTransport }}
            - name: ETCDCTL_CACERT
              value: /var/contiv/etcd-secrets/{{ .Values.etcd.secrets.caCert }}
            - name: ETCDCTL_CERT
              value: /var/contiv/etcd-secrets/{{ .Values.etcd.secrets.clientCert }}
            - name: ETCDCTL_KEY
              value: /var/contiv/etcd-secrets/{{ .Values.etcd.secrets.clientKey }}
            {{- end }}
          command:
            - /bin/sh
          args:
            - -c
            - /usr/local/bin/etcd --name=contiv-etcd --data-dir=/var/etcd/contiv-data
            {{- if .Values.etcd.secureTransport }}
              --client-cert-auth --trusted-ca-file=/var/contiv/etcd-secrets/{{ .Values.etcd.secrets.caCert }}
              --cert-file=/var/contiv/etcd-secrets/{{ .Values.etcd.secrets.serverCert }} --key-file=/var/contiv/etcd-secrets/{{ .Values.etcd.secrets.serverKey }}
              --peer-client-cert-auth --peer-trusted-ca-file=/var/contiv/etcd-secrets/{{ .Values.etcd.secrets.caCert }}
              --peer-cert-file=/var/contiv/etcd-secrets/{{ .Values.etcd.secrets.serverCert }} --peer-key-file=/var/contiv/etcd-secrets/{{ .Values.etcd.secrets.serverKey }}
              --advertise-client-urls=https://0.0.0.0:12379 --listen-client-urls=https://0.0.0.0:12379 --listen-peer-urls=https://0.0.0.0:12380
            {{- else }}
              --advertise-client-urls=http://0.0.0.0:12379 --listen-client-urls=http://0.0.0.0:12379 --listen-peer-urls=http://0.0.0.0:12380
            {{- end }}
          volumeMounts:
            - name: var-etcd
              mountPath: /var/etcd/
            {{- if .Values.etcd.secureTransport }}
            - name: etcd-secrets
              mountPath: /var/contiv/etcd-secrets
              readOnly: true
            {{- end }}
          livenessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - etcdctl get --endpoints=127.0.0.1:{{ .Values.etcd.service.nodePort }} /
            periodSeconds: 2
            initialDelaySeconds: 20
      {{- if .Values.etcd.secureTransport }}
      volumes:
        - name: etcd-secrets
          {{- if .Values.etcd.secrets.mountFromHost }}
          hostPath:
            path: {{ .Values.etcd.secrets.mountDir }}
          {{- else }}
          secret:
            secretName: contiv-etcd-secrets
            items:
            - key: caCert
              path: {{ .Values.etcd.secrets.caCert }}
            - key: serverCert
              path: {{ .Values.etcd.secrets.serverCert }}
            - key: serverKey
              path: {{ .Values.etcd.secrets.serverKey }}
            - key: clientCert
              path: {{ .Values.etcd.secrets.clientCert }}
            - key: clientKey
              path: {{ .Values.etcd.secrets.clientKey }}
          {{- end }}
      {{- end }}
{{- if .Values.etcd.usePersistentVolume }}
  volumeClaimTemplates:
  - metadata:
      name: var-etcd
    spec:
      accessModes:
        - ReadWriteOnce
      resources:
        requests:
          storage: {{ .Values.etcd.persistentVolumeSize }}
    {{- if .Values.etcd.persistentVolumeStorageClass }}
    {{- if (eq "-" .Values.etcd.persistentVolumeStorageClass) }}
      storageClassName: ""
    {{- else }}
      storageClassName: "{{ .Values.etcd.persistentVolumeStorageClass }}"
    {{- end }}
    {{- end }}
{{- else }}
      {{- if not .Values.etcd.secureTransport }}
      volumes:
      {{- end }}
        - name: var-etcd
          hostPath:
             path: {{ .Values.etcd.dataDir }}
{{- end }}


{{- if .Values.etcd.secureTransport }}
{{- if not .Values.etcd.secrets.mountFromHost }}
---

# The following contains k8s Secrets for use with a TLS enabled etcd cluster.
# For information on populating Secrets, see http://kubernetes.io/docs/user-guide/secrets/
apiVersion: v1
kind: Secret
type: Opaque
metadata:
  name: contiv-etcd-secrets
  namespace: kube-system
data:
  caCert: |-
    {{ .Files.Get .Values.etcd.secrets.caCert | b64enc }}
  serverCert: |-
    {{ .Files.Get .Values.etcd.secrets.serverCert | b64enc }}
  serverKey: |-
    {{ .Files.Get .Values.etcd.secrets.serverKey | b64enc }}
  clientCert: |-
    {{ .Files.Get .Values.etcd.secrets.clientCert | b64enc }}
  clientKey: |-
    {{ .Files.Get .Values.etcd.secrets.clientKey | b64enc }}
{{- end }}
{{- end }}

---

apiVersion: v1
kind: Service
metadata:
  name: contiv-etcd
  namespace: kube-system
spec:
  type: NodePort
  # Match contiv-etcd DaemonSet.
  selector:
    k8s-app: contiv-etcd
  ports:
  - port: 12379
    nodePort: {{ .Values.etcd.service.nodePort }}

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: contiv-ksr-http-cfg
  namespace: kube-system
data:
  http.conf: |
    endpoint: 0.0.0.0:9191
    {{- if .Values.http.enableServerCert }}
    server-cert-file: /var/http/{{ .Values.http.serverCert }}
    server-key-file: /var/http/{{ .Values.http.serverKey }}
    {{- end }}
    {{- if .Values.http.enableBasicAuth }}
    client-basic-auth:
      - {{ .Values.http.basicAuth | quote }}
    {{- end }}

---
# This config map contains ETCD configuration for connecting to the contiv-etcd defined above.
apiVersion: v1
kind: ConfigMap
metadata:
  name: contiv-etcd-cfg
  namespace: kube-system
data:
  etcd.conf: |
    {{- if .Values.etcd.secureTransport }}
    insecure-transport: false
    ca-file: /var/contiv/etcd-secrets/{{ .Values.etcd.secrets.caCert }}
    cert-file: /var/contiv/etcd-secrets/{{ .Values.etcd.secrets.clientCert }}
    key-file: /var/contiv/etcd-secrets/{{ .Values.etcd.secrets.clientKey }}
    {{- else }}
    insecure-transport: true
    {{- end }}
    dial-timeout: 10000000000
    endpoints:
      - "127.0.0.1:{{ .Values.etcd.service.nodePort }}"

---

# This config map contains ETCD configuration for connecting to the contiv-etcd defined above with auto comapact.
apiVersion: v1
kind: ConfigMap
metadata:
  name: contiv-etcd-withcompact-cfg
  namespace: kube-system
data:
  etcd.conf: |
    {{- if .Values.etcd.secureTransport }}
    insecure-transport: false
    ca-file: /var/contiv/etcd-secrets/{{ .Values.etcd.secrets.caCert }}
    cert-file: /var/contiv/etcd-secrets/{{ .Values.etcd.secrets.clientCert }}
    key-file: /var/contiv/etcd-secrets/{{ .Values.etcd.secrets.clientKey }}
    {{- else }}
    insecure-transport: true
    {{- end }}
    dial-timeout: 10000000000
    auto-compact: 600000000000
    endpoints:
      - "127.0.0.1:{{ .Values.etcd.service.nodePort }}"

{{- if .Values.http.enableServerCert }}
{{- if not .Values.http.mountFromHost }}
---

# The following contains k8s Secrets for use with a secured HTTP plugin.
# For information on populating Secrets, see http://kubernetes.io/docs/user-guide/secrets/
apiVersion: v1
kind: Secret
type: Opaque
metadata:
  name: contiv-http-secrets
  namespace: kube-system
data:
  serverCert: |-
    {{ .Files.Get .Values.http.serverCert | b64enc }}
  serverKey: |-
    {{ .Files.Get .Values.http.serverKey | b64enc }}
{{- end }}
{{- end }}

---

# This installs contiv-vswitch on each master and worker node in a Kubernetes cluster.
# It consists of the following containers:
#   - contiv-vswitch container: contains VPP and its management agent
#   - contiv-cni container: installs CNI on the host
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: contiv-vswitch
  namespace: kube-system
  labels:
    k8s-app: contiv-vswitch
spec:
  selector:
    matchLabels:
      k8s-app: contiv-vswitch
  updateStrategy:
    type: {{ .Values.vswitch.updateStrategy }}
  template:
    metadata:
      labels:
        k8s-app: contiv-vswitch
      annotations:
        # Marks this pod as a critical add-on.
        scheduler.alpha.kubernetes.io/critical-pod: ''
    spec:
      tolerations:
      # We need this to schedule on the master no matter what else is going on, so tolerate everything.
      - key: ''
        operator: Exists
        effect: ''
      # This likely isn't needed due to the above wildcard, but keep it in for now.
      - key: CriticalAddonsOnly
        operator: Exists
      hostNetwork: true
      hostPID: true

      # Init containers are executed before regular containers, must finish successfully before regular ones are started.
      initContainers:
      # This container installs the Contiv CNI binaries
      # and CNI network config file on each node.
      - name: contiv-cni
        {{- if .Values.Arm64Platform }}
        image: {{ .Values.cni_arm64.image.repository }}:{{ default .Chart.Version .Values.cni.image.tag }}
        {{- else }}
        image: {{ .Values.cni.image.repository }}:{{ default .Chart.Version .Values.cni.image.tag }}
        {{- end }}
        imagePullPolicy: IfNotPresent
        env:
          - name: SLEEP
            value: "false"
        volumeMounts:
          - mountPath: /opt/cni/bin
            name: cni-bin-dir
          - mountPath: /etc/cni/net.d
            name: cni-net-dir
          - mountPath: /var/run/contiv
            name: contiv-run
      {{- if .Values.contiv.crdNodeConfigurationDisabled }}
      # This init container waits until etcd is started
      - name: wait-foretcd
        env:
          - name: ETCDPORT
            value: {{ .Values.etcd.service.nodePort | quote }}
        {{- if .Values.Arm64Platform }}
        image: {{ .Values.init_arm64.image.repository }}:{{ .Values.init.image.tag }}
        {{- else }}
        image: {{ .Values.init.image.repository }}:{{ .Values.init.image.tag }}
        {{- end }}
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -w 2 127.0.0.1:$ETCDPORT; do echo waiting for etcd; sleep 2; done;']
      {{- else }}
      # This init container waits until crd node specific configuration has been applied
      - name: wait-forcrdconfig
        env:
        - name: ETCDPORT
          value: "32379"
        - name: ETCDCTL_API
          value: "3"
        image: quay.io/coreos/etcd:v3.1.10
        command:
        - /bin/sh
        args:
        - -c
        - |
          set -eu
        {{- if .Values.etcd.secureTransport }}
          until /usr/local/bin/etcdctl --endpoints=127.0.0.1:$ETCDPORT get "/vnf-agent/contiv-ksr/k8s/nodeconfig/$HOSTNAME" --prefix=true \
          --cert=/var/contiv/etcd-secrets/{{ .Values.etcd.secrets.clientCert }} --key=/var/contiv/etcd-secrets/{{ .Values.etcd.secrets.clientKey }} --cacert=/var/contiv/etcd-secrets/{{ .Values.etcd.secrets.caCert }}| grep -m 1 "$HOSTNAME";
        {{- else }}
          until /usr/local/bin/etcdctl --endpoints=127.0.0.1:$ETCDPORT get "/vnf-agent/contiv-ksr/k8s/nodeconfig/$HOSTNAME" --prefix=true | grep -m 1 "$HOSTNAME";
        {{- end }}
          do
            echo waiting for crd node config;
            sleep 1
          done
        {{- if .Values.etcd.secureTransport }}
        volumeMounts:
        - name: etcd-secrets
          mountPath: /var/contiv/etcd-secrets
          readOnly: true
        {{- end }}

      {{- end }}
      # This init container extracts/copies VPP LD_PRELOAD libs and default VPP config to the host.
      - name: vpp-init
        {{- if .Values.Arm64Platform }}
        image: {{ .Values.vswitch_arm64.image.repository }}:{{ default .Chart.Version .Values.vswitch.image.tag }}
        {{- else }}
        image: {{ .Values.vswitch.image.repository }}:{{ default .Chart.Version .Values.cni.image.tag }}
        {{- end }}
        imagePullPolicy: {{ .Values.vswitch.image.pullPolicy }}
        command:
        - /bin/sh
        args:
        - -c
        - |
          set -eu
          chmod 700 /run/vpp
          rm -rf /dev/shm/db /dev/shm/global_vm /dev/shm/vpe-api
          if [ ! -e /host/etc/vpp/contiv-vswitch.conf ]; then
              cp /etc/vpp/contiv-vswitch.conf /host/etc/vpp/
          fi
          if [ ! -d /var/run/contiv ]; then
              mkdir /var/run/contiv
          fi
          chmod 700 /var/run/contiv
          rm -f /var/run/contiv/cni.sock
          if ip link show vpp1 >/dev/null 2>&1; then
               ip link del vpp1
          fi
          cp -f /usr/local/bin/vppctl /host/usr/local/bin/vppctl
          {{- if .Values.vswitch.enableCoreDumps }}
          sysctl -w debug.exception-trace=1
          sysctl -w kernel.core_pattern="{{ .Values.vswitch.coreDumpsDir }}/%e-%t"
          ulimit -c unlimited
          echo 2 > /proc/sys/fs/suid_dumpable
          {{- end }}
        resources: {}
        securityContext:
          privileged: true
        volumeMounts:
          - name: usr-local-bin
            mountPath: /host/usr/local/bin
          - name: vpp-lib64
            mountPath: /vpp-lib64/
          - name: vpp-cfg
            mountPath: /host/etc/vpp
          - name: shm
            mountPath: /dev/shm
          - name: vpp-run
            mountPath: /run/vpp
          - name: contiv-run
            mountPath: /var/run/contiv
          {{- if .Values.vswitch.enableCoreDumps }}
          - name: core-dumps
            mountPath: {{ .Values.vswitch.coreDumpsDir }}
          {{- end }}

      containers:
        # Runs contiv-vswitch container on each Kubernetes node.
        # It contains the vSwitch VPP and its management agent.
        - name: contiv-vswitch
          {{- if .Values.Arm64Platform }}
          image: {{ .Values.vswitch_arm64.image.repository }}:{{ default .Chart.Version .Values.vswitch.image.tag }}
          {{- else }}
          image: {{ .Values.vswitch.image.repository }}:{{ default .Chart.Version .Values.cni.image.tag }}
          {{- end }}
          imagePullPolicy: {{ .Values.vswitch.image.pullPolicy }}
          securityContext:
            privileged: true
          ports:
            # readiness + liveness probe
            - containerPort: 9999
          {{- if .Values.vswitch.enableLivenessReadinessProbes }}
          readinessProbe:
            httpGet:
              path: /readiness
              port: 9999
              {{- if .Values.http.enableServerCert }}
              scheme: HTTPS
              {{- end }}
              {{- if .Values.http.enableBasicAuth }}
              httpHeaders:
                - name: Authorization
                  value: "Basic {{ .Values.http.basicAuth | b64enc }}"
              {{- end }}
            periodSeconds: 3
            timeoutSeconds: 2
            failureThreshold: 3
            initialDelaySeconds: 15
          livenessProbe:
            httpGet:
              path: /liveness
              port: 9999
              {{- if .Values.http.enableServerCert }}
              scheme: HTTPS
              {{- end }}
              {{- if .Values.http.enableBasicAuth }}
              httpHeaders:
                - name: Authorization
                  value: "Basic {{ .Values.http.basicAuth | b64enc }}"
              {{- end }}
            periodSeconds: 3
            timeoutSeconds: 2
            failureThreshold: 3
            initialDelaySeconds: 60
          {{- end }}
          env:
            - name: MICROSERVICE_LABEL
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: ETCD_CONFIG
              value: "/etc/etcd/etcd.conf"
            - name: BOLT_CONFIG
              value: "/etc/agent/bolt.conf"
            {{- if .Values.bolt.debug }}
            - name: DEBUG_BOLT_CLIENT
              value: "true"
            {{- end}}
            - name: TELEMETRY_CONFIG
              value: "/etc/agent/telemetry.conf"
          volumeMounts:
            - name: var-bolt
              mountPath: /var/bolt
            - name: etcd-cfg
              mountPath: /etc/etcd
              {{- if .Values.etcd.secureTransport }}
            - name: etcd-secrets
              mountPath: /var/contiv/etcd-secrets
              readOnly: true
              {{- end }}
              {{- if .Values.http.enableServerCert }}
            - name: http-secrets
              mountPath: /var/http
              readOnly: true
              {{- end }}
            - name: vpp-cfg
              mountPath: /etc/vpp
            - name: shm
              mountPath: /dev/shm
            - name: dev
              mountPath: /dev
            - name: vpp-run
              mountPath: /run/vpp
            - name: contiv-run
              mountPath: /var/run/contiv
            - name: contiv-plugin-cfg
              mountPath: /etc/agent
            - name: govpp-plugin-cfg
              mountPath: /etc/govpp
            {{- if .Values.vswitch.enableCoreDumps }}
            - name: core-dumps
              mountPath: {{ .Values.vswitch.coreDumpsDir }}
            {{- end }}
          {{- if or .Values.vswitch.defineMemoryLimits .Values.vswitch.cpuLimit }}
          resources:
            limits:
          {{- end }}
            {{- if .Values.vswitch.defineMemoryLimits }}
              {{- if .Values.vswitch.hugePages1giLimit }}
              hugepages-1Gi: {{ .Values.vswitch.hugePages1giLimit }}
              {{- else }}
                {{- if .Values.vswitch.hugePages2miLimit }}
              hugepages-2Mi: {{ .Values.vswitch.hugePages2miLimit }}
                {{- end }}
              {{- end }}
              memory: {{ .Values.vswitch.memoryLimit }}
            {{- end }}
            {{- if .Values.vswitch.cpuLimit }}
              cpu: {{ .Values.vswitch.cpuLimit }}
            requests:
              cpu: {{ .Values.vswitch.cpuLimit }}
            {{- end }}

      volumes:
        # Used to connect to contiv-etcd.
        - name: etcd-cfg
          configMap:
            name: contiv-etcd-cfg
        {{- if .Values.etcd.secureTransport }}
        - name: etcd-secrets
          {{- if .Values.etcd.secrets.mountFromHost }}
          hostPath:
            path: {{ .Values.etcd.secrets.mountDir }}
          {{- else }}
          secret:
            secretName: contiv-etcd-secrets
            items:
            - key: caCert
              path: {{ .Values.etcd.secrets.caCert }}
            - key: clientCert
              path: {{ .Values.etcd.secrets.clientCert }}
            - key: clientKey
              path: {{ .Values.etcd.secrets.clientKey }}
          {{- end }}
        {{- end }}
        {{- if .Values.http.enableServerCert }}
        - name: http-secrets
          {{- if .Values.http.mountFromHost }}
          hostPath:
            path: {{ .Values.http.mountDir }}
          {{- else }}
          secret:
            secretName: contiv-http-secrets
            items:
            - key: serverCert
              path: {{ .Values.http.serverCert }}
            - key: serverKey
              path: {{ .Values.http.serverKey }}
          {{- end }}
        {{- end }}
        # Used to install CNI.
        - name: cni-bin-dir
          hostPath:
            path: /opt/cni/bin
        - name: cni-net-dir
          hostPath:
            path: /etc/cni/net.d
        # VPP startup config folder.
        - name: vpp-cfg
          hostPath:
            path: /etc/vpp
        # To install vppctl.
        - name: usr-local-bin
          hostPath:
            path: /usr/local/bin
        # LD_PRELOAD library.
        - name: vpp-lib64
          hostPath:
            path: /tmp/ldpreload/vpp-lib64
        # /dev mount is required for DPDK-managed NICs on VPP (/dev/uio0) and for shared memory communication with VPP (/dev/shm)
        - name: dev
          hostPath:
            path: /dev
        - name: shm
          hostPath:
            path: /dev/shm
        # For CLI unix socket.
        - name: vpp-run
          hostPath:
            path: /run/vpp
        # For CNI / STN unix domain socket
        - name: contiv-run
          hostPath:
            path: /var/run/contiv
        # Used to configure contiv plugin.
        - name: contiv-plugin-cfg
          configMap:
            name: contiv-agent-cfg
        # Used to configure govpp plugin.
        - name: govpp-plugin-cfg
          configMap:
            name: govpp-cfg
        {{- if .Values.vswitch.enableCoreDumps }}
        # Used for vswitch core dumps
        - name: core-dumps
          hostPath:
            path: {{ .Values.vswitch.coreDumpsDir }}
        {{- end }}
        - name: var-bolt
          hostPath:
            path: {{ .Values.bolt.dataDir }}

---

# This installs the contiv-ksr (Kubernetes State Reflector) on the master node in a Kubernetes cluster.
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: contiv-ksr
  namespace: kube-system
  labels:
    k8s-app: contiv-ksr
spec:
  updateStrategy:
    type: {{ .Values.ksr.updateStrategy }}
  template:
    metadata:
      labels:
        k8s-app: contiv-ksr
      annotations:
        # Marks this pod as a critical add-on.
        scheduler.alpha.kubernetes.io/critical-pod: ''
    spec:
      tolerations:
      # We need this to schedule on the master no matter what else is going on, so tolerate everything.
      - key: ''
        operator: Exists
        effect: ''
      # This likely isn't needed due to the above wildcard, but keep it in for now.
      - key: CriticalAddonsOnly
        operator: Exists
      # Only run this pod on the master.
      nodeSelector:
        node-role.kubernetes.io/master: ""
      hostNetwork: true
      # This grants the required permissions to contiv-ksr.
      serviceAccountName: contiv-ksr

      initContainers:
        # This init container waits until etcd is started
        - name: wait-foretcd
          env:
          - name: ETCDPORT
            value: {{ .Values.etcd.service.nodePort | quote }}
          {{- if .Values.Arm64Platform }}
          image: {{ .Values.init_arm64.image.repository }}:{{ .Values.init.image.tag }}
          {{- else }}
          image: {{ .Values.init.image.repository }}:{{ .Values.init.image.tag }}
          {{- end }}
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until nc -w 2 127.0.0.1:$ETCDPORT; do echo waiting for etcd; sleep 2; done;']

      containers:
        - name: contiv-ksr
          {{- if .Values.Arm64Platform }}
          image: {{ .Values.ksr_arm64.image.repository }}:{{ default .Chart.Version .Values.ksr.image.tag }}
          {{- else }}
          image: {{ .Values.ksr.image.repository }}:{{ default .Chart.Version .Values.ksr.image.tag }}
          {{- end }}
          imagePullPolicy: {{ .Values.ksr.image.pullPolicy }}
          env:
            - name: ETCD_CONFIG
              value: "/etc/etcd/etcd.conf"
            - name: HTTP_CONFIG
              value: "/etc/http/http.conf"
          volumeMounts:
            - name: etcd-cfg
              mountPath: /etc/etcd
            - name: http-cfg
              mountPath: /etc/http
            {{- if .Values.etcd.secureTransport }}
            - name: etcd-secrets
              mountPath: /var/contiv/etcd-secrets
              readOnly: true
            {{- end }}
            {{- if .Values.http.enableServerCert }}
            - name: http-secrets
              mountPath: /var/http
              readOnly: true
            {{- end }}
          readinessProbe:
            httpGet:
              path: /readiness
              port: 9191
              {{- if .Values.http.enableServerCert }}
              scheme: HTTPS
              {{- end }}
              {{- if .Values.http.enableBasicAuth }}
              httpHeaders:
                - name: Authorization
                  value: "Basic {{ .Values.http.basicAuth | b64enc }}"
              {{- end }}
            periodSeconds: 1
            initialDelaySeconds: 10
          livenessProbe:
            httpGet:
              path: /liveness
              port: 9191
              {{- if .Values.http.enableServerCert }}
              scheme: HTTPS
              {{- end }}
              {{- if .Values.http.enableBasicAuth }}
              httpHeaders:
                - name: Authorization
                  value: "Basic {{ .Values.http.basicAuth | b64enc }}"
              {{- end }}
            periodSeconds: 1
            initialDelaySeconds: 30

      volumes:
        # Used to connect to contiv-etcd.
        - name: etcd-cfg
          configMap:
            name: contiv-etcd-withcompact-cfg
        - name: http-cfg
          configMap:
            name: contiv-ksr-http-cfg
        {{- if .Values.etcd.secureTransport }}
        - name: etcd-secrets
          {{- if .Values.etcd.secrets.mountFromHost }}
          hostPath:
            path: {{ .Values.etcd.secrets.mountDir }}
          {{- else }}
          secret:
            secretName: contiv-etcd-secrets
            items:
            - key: caCert
              path: {{ .Values.etcd.secrets.caCert }}
            - key: clientCert
              path: {{ .Values.etcd.secrets.clientCert }}
            - key: clientKey
              path: {{ .Values.etcd.secrets.clientKey }}
          {{- end }}
        {{- end }}
        {{- if .Values.http.enableServerCert }}
        - name: http-secrets
          {{- if .Values.http.mountFromHost }}
          hostPath:
            path: {{ .Values.http.mountDir }}
          {{- else }}
          secret:
            secretName: contiv-http-secrets
            items:
            - key: serverCert
              path: {{ .Values.http.serverCert }}
            - key: serverKey
              path: {{ .Values.http.serverKey }}
          {{- end }}
        {{- end }}

---

# This cluster role defines a set of permissions required for contiv-ksr.
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: contiv-ksr
  namespace: kube-system
rules:
  - apiGroups:
    - ""
    - extensions
    resources:
      - pods
      - namespaces
      - networkpolicies
      - services
      - endpoints
      - nodes
    verbs:
      - watch
      - list

---

# This defines a service account for contiv-ksr.
apiVersion: v1
kind: ServiceAccount
metadata:
  name: contiv-ksr
  namespace: kube-system

---

# This binds the contiv-ksr cluster role with contiv-ksr service account.
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: contiv-ksr
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: contiv-ksr
subjects:
- kind: ServiceAccount
  name: contiv-ksr
  namespace: kube-system

---

# This installs the contiv-crd on the master node in a Kubernetes cluster.
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: contiv-crd
  namespace: kube-system
  labels:
    k8s-app: contiv-crd
spec:
  updateStrategy:
    type: {{ .Values.crd.updateStrategy }}
  template:
    metadata:
      labels:
        k8s-app: contiv-crd
      annotations:
        # Marks this pod as a critical add-on.
        scheduler.alpha.kubernetes.io/critical-pod: ''
    spec:
      tolerations:
      # We need this to schedule on the master no matter what else is going on, so tolerate everything.
      - key: ''
        operator: Exists
        effect: ''
      # This likely isn't needed due to the above wildcard, but keep it in for now.
      - key: CriticalAddonsOnly
        operator: Exists
      # Only run this pod on the master.
      nodeSelector:
        node-role.kubernetes.io/master: ""
      hostNetwork: true
      # This grants the required permissions to contiv-crd.
      serviceAccountName: contiv-crd

      initContainers:
      # This init container waits until etcd is started
      - name: wait-foretcd
        env:
        - name: ETCDPORT
          value: {{ .Values.etcd.service.nodePort | quote }}
        {{- if .Values.Arm64Platform }}
        image: {{ .Values.init_arm64.image.repository }}:{{ .Values.init.image.tag }}
        {{- else }}
        image: {{ .Values.init.image.repository }}:{{ .Values.init.image.tag }}
        {{- end }}
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'until nc -w 2 127.0.0.1:$ETCDPORT; do echo waiting for etcd; sleep 2; done;']

      # This init container copies contiv-netctl tool to the host.
      - name: netctl-init
        {{- if .Values.Arm64Platform }}
        image: {{ .Values.crd_arm64.image.repository }}:{{ default .Chart.Version .Values.crd.image.tag }}
        {{- else }}
        image: {{ .Values.crd.image.repository }}:{{ default .Chart.Version .Values.crd.image.tag }}
        {{- end }}
        imagePullPolicy: {{ .Values.crd.image.pullPolicy }}
        command:
        - /bin/sh
        args:
        - -c
        - |
          echo '#!/bin/sh
          kubectl get pods -n kube-system | \
            grep contiv-crd | \
            cut -d " " -f 1 | \
            xargs -I{} kubectl exec -n kube-system {} \
            /contiv-netctl "$@"' \
          > /host/usr/local/bin/contiv-netctl || true

          chmod +x /host/usr/local/bin/contiv-netctl || true
        volumeMounts:
          - name: usr-local-bin
            mountPath: /host/usr/local/bin

      containers:
      - name: contiv-crd
        {{- if .Values.Arm64Platform }}
        image: {{ .Values.crd_arm64.image.repository }}:{{ default .Chart.Version .Values.crd.image.tag }}
        {{- else }}
        image: {{ .Values.crd.image.repository }}:{{ default .Chart.Version .Values.crd.image.tag }}
        {{- end }}
        imagePullPolicy: {{ .Values.crd.image.pullPolicy }}
        env:
        - name: ETCD_CONFIG
          value: "/etc/etcd/etcd.conf"
        - name: HTTP_CONFIG
          value: "/etc/http/http.conf"
        - name: HTTP_CLIENT_CONFIG
          value: "/etc/http/http.client.conf"
        volumeMounts:
        - name: etcd-cfg
          mountPath: /etc/etcd
        - name: http-cfg
          mountPath: /etc/http
        {{- if .Values.etcd.secureTransport }}
        - name: etcd-secrets
          mountPath: /var/contiv/etcd-secrets
          readOnly: true
        {{- end }}
        {{- if .Values.http.enableServerCert }}
        - name: http-secrets
          mountPath: /var/http
          readOnly: true
        {{- end }}
        readinessProbe:
          httpGet:
            path: /readiness
            port: 9090
            {{- if .Values.http.enableServerCert }}
            scheme: HTTPS
            {{- end }}
            {{- if .Values.http.enableBasicAuth }}
            httpHeaders:
              - name: Authorization
                value: "Basic {{ .Values.http.basicAuth | b64enc }}"
            {{- end }}
          periodSeconds: 1
          initialDelaySeconds: 10
        livenessProbe:
          httpGet:
            path: /liveness
            port: 9090
            {{- if .Values.http.enableServerCert }}
            scheme: HTTPS
            {{- end }}
            {{- if .Values.http.enableBasicAuth }}
            httpHeaders:
              - name: Authorization
                value: "Basic {{ .Values.http.basicAuth | b64enc }}"
            {{- end }}
          periodSeconds: 1
          initialDelaySeconds: 30
      volumes:
        # Used to connect to contiv-etcd.
        - name: etcd-cfg
          configMap:
            name: contiv-etcd-cfg
        - name: usr-local-bin
          hostPath:
            path: /usr/local/bin
        - name: http-cfg
          configMap:
            name: contiv-crd-http-cfg
        {{- if .Values.etcd.secureTransport }}
        - name: etcd-secrets
          {{- if .Values.etcd.secrets.mountFromHost }}
          hostPath:
            path: {{ .Values.etcd.secrets.mountDir }}
          {{- else }}
          secret:
            secretName: contiv-etcd-secrets
            items:
            - key: caCert
              path: {{ .Values.etcd.secrets.caCert }}
            - key: clientCert
              path: {{ .Values.etcd.secrets.clientCert }}
            - key: clientKey
              path: {{ .Values.etcd.secrets.clientKey }}
          {{- end }}
        {{- end }}
        {{- if .Values.http.enableServerCert }}
        - name: http-secrets
          {{- if .Values.http.mountFromHost }}
          hostPath:
            path: {{ .Values.http.mountDir }}
          {{- else }}
          secret:
            secretName: contiv-http-secrets
            items:
            - key: serverCert
              path: {{ .Values.http.serverCert }}
            - key: serverKey
              path: {{ .Values.http.serverKey }}
          {{- end }}
        {{- end }}
---

# This cluster role defines a set of permissions required for contiv-crd.
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: contiv-crd
  namespace: kube-system
rules:
- apiGroups:
  - apiextensions.k8s.io
  - nodeconfig.contiv.vpp
  - telemetry.contiv.vpp
  resources:
  - customresourcedefinitions
  - telemetryreports
  - nodeconfigs
  verbs:
  - "*"

---

# This defines a service account for contiv-crd.
apiVersion: v1
kind: ServiceAccount
metadata:
  name: contiv-crd
  namespace: kube-system

---

# This binds the contiv-crd cluster role with contiv-crd service account.
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: contiv-crd
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: contiv-crd
subjects:
- kind: ServiceAccount
  name: contiv-crd
  namespace: kube-system

---

apiVersion: v1
kind: ConfigMap
metadata:
  name: contiv-crd-http-cfg
  namespace: kube-system
data:
  http.conf: |
    endpoint: 0.0.0.0:9090
    {{- if .Values.http.enableServerCert }}
    server-cert-file: /var/http/{{ .Values.http.serverCert }}
    server-key-file: /var/http/{{ .Values.http.serverKey }}
    {{- end }}
    {{- if .Values.http.enableBasicAuth }}
    client-basic-auth:
      - {{ .Values.http.basicAuth | quote }}
    {{- end }}
  http.client.conf: |
    port: 9999
    {{- if .Values.http.enableBasicAuth }}
    basic-auth: {{ .Values.http.basicAuth | quote }}
    {{- end }}
    use-https: {{ .Values.http.enableServerCert }}
