# -*- mode: ruby -*-
# vi: set ft=ruby :

require 'fileutils'

BEGIN {
  STATEFILE = ".vagrant-state"

  # if there's a state file, set all the envvars in the current environment
  if File.exist?(STATEFILE)
    File.read(STATEFILE).lines.map { |x| x.split("=", 2) }.each { |x,y| ENV[x] = y.strip }
  end
}

module VagrantPlugins
  module EnvState
    class Plugin < Vagrant.plugin('2')
    name 'EnvState'

    def self.up_hook(arg)
        unless File.exist?(STATEFILE)
        f = File.open(STATEFILE, "w")
        ENV.each do |x,y|
            f.puts "%s=%s" % [x,y]
        end
        f.close
        end
    end

    def self.destroy_hook(arg)
        if File.exist?(STATEFILE)
            File.unlink(STATEFILE)
        end
    end

    action_hook(:EnvState, :machine_action_up) do |hook|
        hook.prepend(method(:up_hook))
    end

    action_hook(:EnvState, :machine_action_destroy) do |hook|
        hook.prepend(method(:destroy_hook))
    end
    end
  end
end

# SET ENV
http_proxy = ENV['HTTP_PROXY'] || ENV['http_proxy'] || ''
https_proxy = ENV['HTTPS_PROXY'] || ENV['https_proxy'] || ''
node_os = ENV['K8S_NODE_OS'] || 'ubuntu'
base_ip = ENV['K8S_IP_PREFIX'] || '10.20.0.'
num_nodes = ENV['K8S_NODES'].to_i == 0 ? 0 : ENV['K8S_NODES'].to_i
provider = ENV['VAGRANT_DEFAULT_PROVIDER']
dep_env = ENV['K8S_DEPLOYMENT_ENV']
dep_scenario = ENV['K8S_DEPLOYMENT_SCENARIO']
image_tag = ENV['CONTIV_IMAGE_TAG']

provision_every_node = <<SCRIPT
set -e
set -x
# setup the environment file. Export the env-vars passed as args to 'vagrant up'
# This script will also: add keys, update and install pre-requisites

echo Args passed: [[ $@ ]]

cat <<EOF >/etc/profile.d/envvar.sh
export http_proxy='#{http_proxy}'
export https_proxy='#{https_proxy}'
EOF

source /etc/profile.d/envvar.sh
echo "Updating apt lists..."
sudo apt-get update

echo "Installing dependency packages..."
sudo apt-get install -y apt-transport-https \
                   ca-certificates \
                   curl \
                   software-properties-common \
                   htop 
                   
echo "Adding Kubernetes & Docker repos..."
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -
cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
deb http://apt.kubernetes.io/ kubernetes-xenial main
deb [arch=amd64] https://download.docker.com/linux/ubuntu xenial stable
EOF

echo "Updating apt lists..."
sudo apt-get update

echo "Installing Kubernetes Components..."
sudo apt-get install -y kubelet kubectl kubeadm kubernetes-cni

echo "Installing Docker-CE..."
sudo apt-get install -y docker-ce
systemctl stop docker
modprobe overlay

echo '{"storage-driver": "overlay2"}' > /etc/docker/daemon.json
rm -rf /var/lib/docker/*
systemctl start docker


if [ "$3" == "dev" ]; then 
  export GO_VERSION=1.9.3
  echo "Downloading Go $GO_VERSION..."
  curl --silent https://storage.googleapis.com/golang/go$GO_VERSION.linux-amd64.tar.gz > /tmp/go.tar.gz

  echo "Extracting Go..."
  tar -xvzf /tmp/go.tar.gz --directory /home/vagrant >/dev/null 2>&1

  echo "Setting Go environment variables..."
  mkdir -p /home/vagrant/gopath/bin
  mkdir -p /home/vagrant/gopath/pkg
  chmod -R 777 /home/vagrant/gopath

  echo 'export GOROOT="/home/vagrant/go"' >> /home/vagrant/.bashrc
  echo 'export GOPATH="/home/vagrant/gopath"' >> /home/vagrant/.bashrc
  echo 'export PATH="$PATH:$GOROOT/bin:$GOPATH/bin"' >> /home/vagrant/.bashrc

  source /home/vagrant/.bashrc
  update-locale LANG=en_US.UTF-8 LANGUAGE=en_US.UTF-8 LC_ALL=en_US.UTF-8
  echo 'All done!'
fi

#Disable swap
swapoff -a
sed -e '/swap/ s/^#*/#/' -i /etc/fstab
SCRIPT

vbox_provision_every_node = <<SCRIPT
set -e
set -x
#Load uio_pci_generic driver and setup the loading on each boot up
installPCIUIO() {
   modprobe uio_pci_generic
      # check if the driver is not already added into the file
      if ! grep -q "uio_pci_generic" /etc/modules; then
         echo uio_pci_generic >> /etc/modules
         echo "Module uio_pci_generic was added into /etc/modules"
      fi
}
#Selects an interface that will be used for node interconnect
createVPPconfig() {
mkdir -p /etc/vpp
touch /etc/vpp/contiv-vswitch.conf
  cat <<EOF >/etc/vpp/contiv-vswitch.conf
unix {
   nodaemon
   cli-listen /run/vpp/cli.sock
   cli-no-pager
}
cpu {
    workers 1
}
dpdk {
   dev 0000:00:08.0
}
nat {
   endpoint-dependent
}
api-trace {
   on
   nitems 500
}
EOF
}
kernelModule=$(lsmod | grep uio_pci_generic | wc -l)
if [[ $kernelModule -gt 0 ]]; then
    echo "PCI UIO driver is loaded"
else
    installPCIUIO
fi
createVPPconfig
if [ "$3" = 'nostn' ]; then
    #shutdown interface
    ip link set enp0s8 down
    echo "#auto enp0s8" >> /etc/network/interfaces
fi
SCRIPT

vbox_bootstrap_master = <<SCRIPT
set -e
set -x

echo Args passed: [[ $@ ]]

sudo apt-get install -y python-pip \
                   python-dev \
                   python-virtualenv \
                   build-essential \

#Install pip
sudo pip install --upgrade pip
sudo pip install --upgrade virtualenv

if [ -f /vagrant/images.tar ]; then
    echo "Found saved images at /vagrant/images.tar"
    docker load -i /vagrant/images.tar
else
  # Pull images
  echo "Pulling contiv-vpp plugin images..."
  /home/vagrant/gopath/src/github.com/contiv/vpp/k8s/pull-images.sh -b $6
fi

#Install helm
export HELM_VERSION=2.9.1
curl -sL https://storage.googleapis.com/kubernetes-helm/helm-v$HELM_VERSION-linux-amd64.tar.gz > /tmp/helm.tgz
tar -zxvf /tmp/helm.tgz -C /tmp
mv /tmp/linux-amd64/helm /usr/local/bin/helm

if [ "$4" = "dev" ]; then
# --------------------------------------------------------
# ---> Build Contiv/VPP-vswitch Development Image <---
# --------------------------------------------------------
    echo "vagrant" >> /home/vagrant/gopath/src/github.com/contiv/vpp/.dockerignore
    echo "Building development contivpp/vswitch image..."
    cd /home/vagrant/gopath/src/github.com/contiv/vpp/docker; ./build-all.sh
fi

# --------------------------------------------------------
# ---> Create token and export it with kube master IP <---
# --------------------------------------------------------

echo "Exporting Kube Master IP and Kubeadm Token..."
echo "export KUBEADM_TOKEN=$(kubeadm token generate)" >> /vagrant/config/init

if [ $5 = 'stn' ]; then
  echo "export KUBE_MASTER_IP=$(hostname -I | cut -f2 -d' ')" >> /vagrant/config/init
  source /vagrant/config/init
  sed 's/127\.0\.0\.1.*k8s.*/'"$KUBE_MASTER_IP"' '"$1"'/' -i /etc/hosts
  echo "export no_proxy='$1,$KUBE_MASTER_IP,localhost,127.0.0.1'" >> /etc/profile.d/envvar.sh
  echo "export no_proxy='$1,$KUBE_MASTER_IP,localhost,127.0.0.1'" >> /home/vagrant/.profile
else 
  echo "export KUBE_MASTER_IP=$2" >> /vagrant/config/init
  source /vagrant/config/init
  sed 's/127\.0\.0\.1.*k8s.*/'"$2"' '"$1"'/' -i /etc/hosts
  echo "export no_proxy='$1,$KUBE_MASTER_IP,localhost,127.0.0.1'" >> /etc/profile.d/envvar.sh
  echo "export no_proxy='$1,$KUBE_MASTER_IP,localhost,127.0.0.1'" >> /home/vagrant/.profile
fi

source /etc/profile.d/envvar.sh
source /home/vagrant/.profile

# --------------------------------------------------------
# --------------> Kubeadm & Networking <------------------
# --------------------------------------------------------

sed -i '4 a Environment="KUBELET_EXTRA_ARGS=--node-ip='"$KUBE_MASTER_IP"' --feature-gates HugePages=false"' /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
systemctl daemon-reload
systemctl restart kubelet
echo "$(kubeadm init --token-ttl 0 --pod-network-cidr="10.0.0.0/8" --apiserver-advertise-address="${KUBE_MASTER_IP}" --token="${KUBEADM_TOKEN}")" >> /vagrant/config/cert

echo "Create folder to store kubernetes and network configuration"
mkdir -p /home/vagrant/.kube
sudo cp -i /etc/kubernetes/admin.conf /home/vagrant/.kube/config
sudo chown vagrant:vagrant -R /home/vagrant/.kube

echo "Installing Contiv-VPP networking as user"
sudo -u vagrant -H bash << EOF

echo "Installing Pod Network..."

if [ $6 != "latest" ]; then
  sed -i -e "s/latest/$6/" /home/vagrant/gopath/src/github.com/contiv/vpp/k8s/contiv-vpp/Chart.yaml
fi
helm template --name vagrant /home/vagrant/gopath/src/github.com/contiv/vpp/k8s/contiv-vpp > /home/vagrant/gopath/src/github.com/contiv/vpp/k8s/contiv-vpp/manifest.yaml
export NUM_K8S_NODES=$3; export DEP_SCENARIO=$5; /home/vagrant/gopath/src/github.com/contiv/vpp/vagrant/config/vagrant-yaml
kubectl apply -f /home/vagrant/gopath/src/github.com/contiv/vpp/k8s/contiv-vpp/manifest.yaml

echo "Schedule Pods on master"
kubectl taint nodes --all node-role.kubernetes.io/master-
EOF
SCRIPT

vbox_bootstrap_worker = <<SCRIPT
set -e
set -x

echo Args passed: [[ $@ ]]

if [ -f /vagrant/images.tar ]; then
    echo "Found saved images at /vagrant/images.tar"
    docker load -i /vagrant/images.tar
fi

source /vagrant/config/init

if [ $3 = 'stn' ]; then
  export KUBE_WORKER_IP=$(hostname -I | cut -f2 -d' ')
else 
  export KUBE_WORKER_IP=$2
fi

sed -i '4 a Environment="KUBELET_EXTRA_ARGS=--node-ip='"$KUBE_WORKER_IP"' --feature-gates HugePages=false"' /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
systemctl daemon-reload
systemctl restart kubelet

sed 's/127\.0\.0\.1.*k8s.*/'"$KUBE_WORKER_IP"' '"$1"'/' -i /etc/hosts
echo "export no_proxy='$1,$KUBE_MASTER_IP,$KUBE_WORKER_IP,localhost,127.0.0.1'" >> /etc/profile.d/envvar.sh
echo "export no_proxy='$1,$KUBE_MASTER_IP,$KUBE_WORKER_IP,localhost,127.0.0.1'" >> /home/vagrant/.profile
source /etc/profile.d/envvar.sh
source /home/vagrant/.profile

if [ "$3" = 'stn' ]; then
  curl -s https://raw.githubusercontent.com/contiv/vpp/master/k8s/stn-install.sh > /tmp/contiv-stn.sh
  chmod +x /tmp/contiv-stn.sh
  sudo /tmp/contiv-stn.sh
fi

hash=$(awk 'END {print $NF}' /vagrant/config/cert)
kubeadm join --token "${KUBEADM_TOKEN}"  "${KUBE_MASTER_IP}":6443 --discovery-token-ca-cert-hash "$hash"
SCRIPT

vbox_provision_gateway = <<SCRIPT
set -e
set -x

echo Args passed: [[ $@ ]]

sed -i '/net.ipv4.ip_forward/s/^#//g' /etc/sysctl.conf
sysctl -p /etc/sysctl.conf

iptables --table nat --append POSTROUTING --out-interface enp0s3 -j MASQUERADE
iptables --append FORWARD --in-interface enp0s8 -j ACCEPT

# Load iptables rules on boot.
iptables-save >/etc/iptables-rules-v4.conf
cat<<'EOF'>/etc/network/if-pre-up.d/iptables-restore
#!/bin/sh
iptables-restore </etc/iptables-rules-v4.conf
EOF

chmod +x /etc/network/if-pre-up.d/iptables-restore
SCRIPT

VAGRANTFILE_API_VERSION = "2"
  Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|
    config.vm.box_check_update = false
    if Vagrant.has_plugin?("vagrant-vbguest")
        config.vbguest.auto_update = false
    end
    if node_os == "ubuntu" then
        config.vm.box = "puppetlabs/ubuntu-16.04-64-nocm"
        config.vm.box_version = "1.0.0"
    else
        # Nothing for now, later add more OS
    end

    node_ips = num_nodes.times.collect { |n| base_ip + "#{n+10}" }
    node_names = num_nodes.times.collect { |n| "k8s-worker#{n+1}" }
    config.ssh.insert_key = false

    if Vagrant.has_plugin?("vagrant-cachier")
      config.cache.scope = :box
      config.cache.enable :apt
    end
    config.vm.provider 'virtualbox' do |v|
      v.linked_clone = true if Vagrant::VERSION >= "1.8"
      v.customize ['modifyvm', :id, '--paravirtprovider', 'kvm']
    end

    #Configure VBox Gateway
    config.vm.define "k8s-gateway" do |gw|
      gw.vm.hostname = "k8s-gateway"
      # Interface for K8s Cluster
      gw.vm.network :private_network, ip: "192.168.16.100", virtualbox__intnet: "vpp"
      gw.vm.provider "virtualbox" do |v|
        v.customize ["modifyvm", :id, "--ioapic", "on"]
        v.memory = 512
        v.cpus = 1
      end
      gw.vm.provision "shell" do |s|
        s.inline = vbox_provision_gateway
        s.args = [http_proxy, https_proxy]
      end
    end

    # Configure VBox Master node
    config.vm.define "k8s-master" do |k8smaster|
      k8smaster.vm.host_name = "k8s-master"
      k8smaster_ip = base_ip + "2"
      k8smaster.vm.synced_folder "../", "/home/vagrant/gopath/src/github.com/contiv/vpp"

      if dep_scenario == 'stn'
        k8smaster.vm.network :private_network, type: "dhcp", auto_config: true, virtualbox__intnet: "vpp"
        # default router
        k8smaster.vm.provision "shell",
          run: "always",
          inline: "route add default gw 10.20.0.100"
        # delete default gw on eth0
        k8smaster.vm.provision "shell",
          run: "always",
          inline: "eval `route -n | awk '{ if ($8 ==\"enp0s3\" && $2 != \"0.0.0.0\") print \"route del default gw \" $2; }'`"
      else
        k8smaster.vm.network :private_network, type: "dhcp", auto_config: false, virtualbox__intnet: "vpp"
        k8smaster.vm.network :private_network, ip: k8smaster_ip, virtualbox__intnet: "true"
      end

      k8smaster.vm.provider "virtualbox" do |v|
        v.customize ["modifyvm", :id, "--ioapic", "on"]
        v.memory = 2048
        v.cpus = 2
      end
      k8smaster.vm.provision "shell" do |s|
        s.inline = provision_every_node
        s.args = [http_proxy, https_proxy, dep_env, num_nodes]
      end
      k8smaster.vm.provision "shell" do |s|
        s.inline = vbox_provision_every_node
        s.args = [http_proxy, https_proxy, dep_scenario]
      end
      k8smaster.vm.provision "shell" do |s|
        s.inline = vbox_bootstrap_master
        s.args = ["k8s-master", k8smaster_ip, num_nodes, dep_env, dep_scenario, image_tag]
      end
    end

    # Configure VBox Worker node(s)
    num_nodes.times do |n|
      node_name = node_names[n]
      node_addr = node_ips[n]

      config.vm.define node_name do |node|
        node.vm.hostname = node_name
        # Interface for K8s Cluster
        if dep_scenario == 'stn'
          node.vm.network :private_network, type: "dhcp", auto_config: true, virtualbox__intnet: "vpp"
          # default router
          node.vm.provision "shell",
            run: "always",
            inline: "route add default gw 10.20.0.100"
          # delete default gw on eth0
          node.vm.provision "shell",
            run: "always",
            inline: "eval `route -n | awk '{ if ($8 ==\"enp0s3\" && $2 != \"0.0.0.0\") print \"route del default gw \" $2; }'`"
        else
          node.vm.network :private_network, type: "dhcp", auto_config: false, virtualbox__intnet: "vpp"
          node.vm.network :private_network, ip: node_addr, virtualbox__intnet: "true"
        end

        node.vm.provider "virtualbox" do |v|
          v.customize ["modifyvm", :id, "--ioapic", "on"]
          v.memory = 2048
          v.cpus = 2
        end
        node.vm.provision "shell" do |s|
          s.inline = provision_every_node
          s.args = [http_proxy, https_proxy, dep_env]
        end
        node.vm.provision "shell" do |s|
          s.inline = vbox_provision_every_node
          s.args = [http_proxy, https_proxy, dep_scenario]
        end
        node.vm.provision "shell" do |s|
          s.inline = vbox_bootstrap_worker
          s.args = [node_name, node_addr, dep_scenario]
        end
      end
  end
end
