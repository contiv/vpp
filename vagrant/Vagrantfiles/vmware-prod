# -*- mode: ruby -*-
# vi: set ft=ruby :

require 'fileutils'

BEGIN {
  STATEFILE = ".vagrant-state"

  # if there's a state file, set all the envvars in the current environment
  if File.exist?(STATEFILE)
    File.read(STATEFILE).lines.map { |x| x.split("=", 2) }.each { |x,y| ENV[x] = y.strip }
  end
}

module VagrantPlugins
    module EnvState
        class Plugin < Vagrant.plugin('2')
        name 'EnvState'

        def self.up_hook(arg)
            unless File.exist?(STATEFILE)
            f = File.open(STATEFILE, "w") 
            ENV.each do |x,y|
                f.puts "%s=%s" % [x,y]
            end
            f.close
            end
        end

        def self.destroy_hook(arg)
            if File.exist?(STATEFILE)
                File.unlink(STATEFILE)
            end
        end

        action_hook(:EnvState, :machine_action_up) do |hook|
            hook.prepend(method(:up_hook))
        end

        action_hook(:EnvState, :machine_action_destroy) do |hook|
            hook.prepend(method(:destroy_hook))
        end
        end
    end
end

# SET ENV
http_proxy = ENV['HTTP_PROXY'] || ENV['http_proxy'] || ''
https_proxy = ENV['HTTPS_PROXY'] || ENV['https_proxy'] || ''
node_os = ENV['K8S_NODE_OS'] || 'ubuntu'
num_nodes = ENV['K8S_NODES'].to_i == 0 ? 0 : ENV['K8S_NODES'].to_i

provision_every_node = <<SCRIPT
set -e
set -x
## setup the environment file. Export the env-vars passed as args to 'vagrant up'
# This script will also: add keys, update and install pre-requisites

echo Args passed: [[ $@ ]]
cat <<EOF >/etc/profile.d/envvar.sh
export http_proxy='#{http_proxy}'
export https_proxy='#{https_proxy}'
EOF

source /etc/profile.d/envvar.sh
echo "Updating apt lists..."
apt-get update

echo "Installing dependency packages..."
apt-get install -y apt-transport-https \
                   ca-certificates \
                   curl \
                   software-properties-common

echo "Adding Kubernetes & Docker repos..."
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -
cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
deb http://apt.kubernetes.io/ kubernetes-xenial main
deb [arch=amd64] https://download.docker.com/linux/ubuntu xenial stable
EOF

echo "Updating apt lists..."
apt-get update

echo "Installing Kubernetes Components..."
apt-get install -y kubelet kubectl kubeadm kubernetes-cni

echo "Installing Docker-CE..."
apt-get install -y docker-ce
systemctl stop docker
modprobe overlay

echo '{"storage-driver": "overlay2"}' > /etc/docker/daemon.json
rm -rf /var/lib/docker/*
systemctl start docker

#Load uio_pci_generic driver and setup the loading on each boot up
installPCIUIO() {
   modprobe uio_pci_generic
      # check if the driver is not already added into the file
      if ! grep -q "uio_pci_generic" /etc/modules; then
         echo uio_pci_generic >> /etc/modules
         echo "Module uio_pci_generic was added into /etc/modules"
      fi
}

#Selects an interface that will be used for node interconnect
createVPPconfig() {
mkdir -p /etc/vpp
touch /etc/vpp/contiv-vswitch.conf
  cat <<EOF >/etc/vpp/contiv-vswitch.conf
unix {
   nodaemon
   cli-listen /run/vpp/cli.sock
   cli-no-pager
}
dpdk {
   dev 0000:03:00.0
}
EOF

#shutdown interface
ip link set ens160 down
}

kernelModule=$(lsmod | grep uio_pci_generic | wc -l)
if [[ $kernelModule -gt 0 ]]; then
    echo "PCI UIO driver is loaded"
else
    installPCIUIO
fi
createVPPconfig

echo "#auto ens160" >> /etc/network/interfaces

# Pull the latest images
bash <(curl -s https://raw.githubusercontent.com/contiv/vpp/master/k8s/pull-images.sh)

#Disable swap
swapoff -a
sed -e '/swap/ s/^#*/#/' -i /etc/fstab
SCRIPT

bootstrap_master = <<SCRIPT
set -e
set -x

echo Args passed: [[ $@ ]]

# --------------------------------------------------------
# ---> Create token and export it with kube master IP <---
# --------------------------------------------------------

echo "Exporting Kube Master IP and Kubeadm Token..."
echo "export KUBE_MASTER_IP=$(hostname -I | cut -f1 -d' ')" >> /vagrant/config/init
echo "export KUBEADM_TOKEN=$(kubeadm token generate)" >> /vagrant/config/init
source /vagrant/config/init
sed 's/127\.0\.0\.1.*k8s.*/'"$KUBE_MASTER_IP"' '"$1"'/' -i /etc/hosts

echo "export no_proxy='$1,$KUBE_MASTER_IP,localhost,127.0.0.1'" >> /etc/profile.d/envvar.sh
echo "export no_proxy='$1,$KUBE_MASTER_IP,localhost,127.0.0.1'" >> /home/vagrant/.profile
source /etc/profile.d/envvar.sh
source /home/vagrant/.profile

# --------------------------------------------------------
# --------------> Kubeadm & Networking <------------------
# --------------------------------------------------------

sed -i '4 a Environment="KUBELET_EXTRA_ARGS=--node-ip=$KUBE_MASTER_IP"' /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
systemctl daemon-reload
systemctl restart kubelet
curl --silent https://raw.githubusercontent.com/contiv/vpp/master/k8s/contiv-vpp.yaml > /tmp/contiv-vpp.yaml
POD_SUBNET_CIDR=$(grep -A0 'PodSubnetCIDR:' /tmp/contiv-vpp.yaml | tail -n1 | awk '{ print $2}')
echo "$(kubeadm init --token-ttl 0 --pod-network-cidr="${POD_SUBNET_CIDR}" --apiserver-advertise-address="${KUBE_MASTER_IP}" --token="${KUBEADM_TOKEN}")" >> /vagrant/config/cert

echo "Create folder to store kubernetes and network configuration"
mkdir -p /home/vagrant/.kube
sudo cp -i /etc/kubernetes/admin.conf /home/vagrant/.kube/config
sudo chown vagrant:vagrant -R /home/vagrant/.kube

echo "Installing Contiv-VPP networking as user"
sudo -u vagrant -H bash << EOF

echo "Installing Pod Network..."
kubectl apply -f https://raw.githubusercontent.com/contiv/vpp/master/k8s/contiv-vpp.yaml

echo "Schedule Pods on master"
kubectl taint nodes --all node-role.kubernetes.io/master-
EOF
SCRIPT

bootstrap_worker = <<SCRIPT
set -e
set -x

echo Args passed: [[ $@ ]]

source /vagrant/config/init
export KUBE_WORKER_IP=$(hostname -I | cut -f1 -d' ')
sed 's/127\.0\.0\.1.*k8s.*/'"$KUBE_WORKER_IP"' '"$1"'/' -i /etc/hosts
echo "export no_proxy='$1,$KUBE_MASTER_IP,$KUBE_WORKER_IP,localhost,127.0.0.1'" >> /etc/profile.d/envvar.sh
echo "export no_proxy='$1,$KUBE_MASTER_IP,$KUBE_WORKER_IP,localhost,127.0.0.1'" >> /home/vagrant/.profile
source /etc/profile.d/envvar.sh
source /home/vagrant/.profile

# Kubeadm join expects kube_master_ip and kubeadm_token
hash=$(awk 'END {print $NF}' /vagrant/config/cert)
kubeadm join --token "${KUBEADM_TOKEN}"  "${KUBE_MASTER_IP}":6443 --discovery-token-ca-cert-hash "$hash"
SCRIPT

VAGRANTFILE_API_VERSION = "2"
Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|
    config.vm.box_check_update = false
    if Vagrant.has_plugin?("vagrant-vbguest")
        config.vbguest.auto_update = false
    end 
    if node_os == "ubuntu" then
        config.vm.box = "puppetlabs/ubuntu-16.04-64-nocm"
        config.vm.box_version = "1.0.0"
    else
        # Nothing for now, later add more OS
    end
    config.vm.provider 'vmware_fusion' do |v|
        v.linked_clone = true if Vagrant::VERSION >= "1.8"
    end

    config.ssh.insert_key = false
    node_names = num_nodes.times.collect { |n| "k8s-worker#{n+1}" }

    if Vagrant.has_plugin?("vagrant-cachier")
        config.cache.scope = :box
        config.cache.enable :apt
    end
    
    # Configure Master node
    config.vm.define "k8s-master" do |k8smaster|
        k8smaster.vm.host_name = "k8s-master"
        k8smaster.vm.network :private_network, type: "dhcp", auto_config: false
        k8smaster.vm.provider "vmware_fusion" do |v|
          v.vmx["memsize"] = "2048"
          v.vmx["numvcpus"] = "2"
          v.vmx["ethernet1.virtualdev"] = "vmxnet3"
        end
        k8smaster.vm.provision "shell" do |s|
            s.inline = provision_every_node
            s.args = [http_proxy, https_proxy]
        end
        k8smaster.vm.provision "shell" do |s|
            s.inline = bootstrap_master
            s.args = ["k8s-master"]
        end
    end

    num_nodes.times do |n|
        node_name = node_names[n]

        config.vm.define node_name do |node|
            node.vm.hostname = node_name
            # Interface for K8s Cluster
            node.vm.network :private_network, type: "dhcp", auto_config: false
            node.vm.provider "vmware_fusion" do |v|
              v.vmx["memsize"] = "2048"
              v.vmx["numvcpus"] = "2"
              v.vmx["ethernet1.virtualdev"] = "vmxnet3"
            end
            node.vm.provision "shell" do |s|
                s.inline = provision_every_node
                s.args = [http_proxy, https_proxy]
            end
            node.vm.provision "shell" do |s|
                s.inline = bootstrap_worker
                s.args = [node_name]
            end
        end
    end
end
